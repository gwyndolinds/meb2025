{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimación Montecarlo\n",
    "\n",
    "<img style=\"float: right; margin: 0px 0px 15px 15px;\" src=\"https://upload.wikimedia.org/wikipedia/commons/b/bf/Simple_random_sampling.PNG\" width=\"400px\" height=\"400px\" />\n",
    "\n",
    "> Hasta ahora, hemos desarrollado métodos de inferencia exacta.\n",
    ">\n",
    "> Muchas veces, las formas de las distribuciones son complejas, y las integrales que corresponden a los valores esperados no son posibles de resolver. Por tanto, en este tema estudiamos un método alternativo para estimar estos valores esperados a partir de muestreo de las distribuciones.\n",
    "\n",
    "> **Objetivos:**\n",
    "> - Entender la necesidad de los métodos Montecarlo en escenarios donde calcular las esperanzas no es viable.\n",
    "> - Comprender métodos de muestreo en distribuciones simples unidimensionales.\n",
    "> - Estimar valores esperados de funciones en distribuciones unidimensionales.\n",
    "\n",
    "> **Referencias:**\n",
    "> - Bayesian Methods for Machine Learning course, HSE University, Coursera.\n",
    "> - Probabilistic Graphical Models: Principles and Techniques, By Daphne Koller and Nir Friedman. Ch. 12.\n",
    "\n",
    "<p style=\"text-align:right;\"> Imagen recuperada de: https://upload.wikimedia.org/wikipedia/commons/b/bf/Simple_random_sampling.PNG.</p>\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introducción\n",
    "\n",
    "<img style=\"float: right; margin: 0px 0px 15px 15px;\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/54/Monte_carlo_method.svg\" width=\"300px\" height=\"300px\" />\n",
    "\n",
    "- Inventado por Stanislaw Ulam y a John von Neumann. Ulam ha explicado cómo se le ocurrió la idea mientras jugaba un solitario durante una enfermedad en 1946. \n",
    "- Advirtió que resulta mucho más simple tener una idea del resultado general del solitario haciendo pruebas múltiples con las cartas y contando las proporciones de los resultados que computar todas las posibilidades de combinación formalmente.\n",
    "- Se le ocurrió que esta misma observación debía aplicarse a su trabajo de Los Álamos sobre difusión de neutrones, para la cual resulta prácticamente imposible solucionar las ecuaciones íntegro-diferenciales que gobiernan la dispersión, la absorción y la fisión.\n",
    "- Dado que ya  empezaban a estar disponibles máquinas de computación para efectuar las pruebas numéricas, el método cobró  mucha fuerza.\n",
    "- El método de Montecarlo proporciona soluciones aproximadas a una gran variedad de problemas matemáticos posibilitando la realización de experimentos con muestreos de números pseudoaleatorios en una computadora. El método es aplicable a cualquier tipo de problema, ya sea estocástico o determinista. \n",
    "- El método de Montecarlo tiene un error absoluto de la estimación que decrece como $\\frac{1}{\\sqrt{N}}$ en virtud del teorema del límite central.\n",
    "\n",
    "\n",
    "Todos alguna vez hemos aplicado el método Montecarlo (inconscientemente). Como ejemplo, consideremos el juego de Astucia Naval.\n",
    "\n",
    "Normalmente, primero se realizan una serie de tiros a puntos aleatorios. Una vez se impacta en un barco, se puede utilizar un algoritmo determinista para identificar la posición del barco y así terminar de derrumbarlo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Par ponerlo en términos sencillos**: muchas veces resolver exactamente un problema (integrales, ecuaciones, entre otros) es una tarea compleja, incluso intratable o no realizable. En esos casos, el método Monte Carlo nos dice:\n",
    "\n",
    "> Simula tu sistema muchas veces, y revisa qué pasa en promedio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo - Estimemos $\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sabemos que el área de un círculo de radio $r$ es:\n",
    "\n",
    "$$\n",
    "A_c(r) = \\pi r^2.\n",
    "$$\n",
    "\n",
    "De manera que el área de un círculo de radio unitario es $A_c(1) = \\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, podemos utilizar este hecho para evaluar el siguiente valor esperado:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{xy}[\\mathbb{1}[x^2 + y^2 \\leq 1]],\n",
    "$$\n",
    "\n",
    "donde $x, y \\sim \\mathcal{U}(0, 1)$, y $\\mathbb{1}$ es la función indicadora:\n",
    "\n",
    "$$\n",
    "\\mathbb{1}[x^2 + y^2 \\leq 1] = \\left\\{\n",
    "\\begin{array}{ccc} 1 & \\text{si} & x^2 + y^2 \\leq 1 \\\\\n",
    "                                                        0 & \\text{de otro modo} &        \\end{array}\\right.\n",
    "$$\n",
    "\n",
    "Por definición tenemos que:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{xy}[\\mathbb{1}[x^2 + y^2 \\leq 1]] = \\int\\int_{[0,1] \\times [0, 1]} \\mathbb{1}[x^2 + y^2 \\leq 1]dx dy,\n",
    "$$\n",
    "\n",
    "y esta integral representa el área del cuarto de círculo unitario en el cuadrante positivo. Es decir:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{xy}[\\mathbb{1}[x^2 + y^2 \\leq 1]] = \\int\\int_{[0,1] \\times [0, 1]} \\mathbb{1}[x^2 + y^2 \\leq 1]dx dy = \\frac{\\pi}{4}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por otra parte, interpretando el valor esperado como un promedio, podemos aproximar:\n",
    "\n",
    "$$\n",
    "\\frac{\\pi}{4} = \\mathbb{E}_{xy}[\\mathbb{1}[x^2 + y^2 \\leq 1]] \\approx \\frac{1}{M} \\sum_{s=1}^{M} \\mathbb{1}[x_s^2 + y_s^2 \\leq 1].\n",
    "$$\n",
    "\n",
    "donde $\\{(x_s, y_s)\\}_{s=1}^{M}$ son ambas realizaciones independientes de la VA uniforme en  (0, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos matplotlib.pyplot\n",
    "\n",
    "# Importamos numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos M puntos uniformes en [0, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráficamente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimación de pi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este hecho que usamos acá (aproximar el valor esperado de una función con el promedio crudo de la función en muestras simuladas): \n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{xy}[\\mathbb{1}[x^2 + y^2 \\leq 1]] \\approx \\frac{1}{M} \\sum_{s=1}^{M} \\mathbb{1}[x_s^2 + y_s^2 \\leq 1],\n",
    "$$\n",
    "\n",
    "se generaliza a cualquier función."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Estimación de valores esperados mediante muestreo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto es,\n",
    "\n",
    "> Dada una distribución $p$ y una función $f$,\n",
    ">\n",
    "> $$\\mathbb{E}_{p(x)}[f(x)] \\approx \\frac{1}{M} \\sum_{s=1}^{M}f(x_s),$$\n",
    ">\n",
    "> donde $\\{x_s\\}_{s=1}^{M}$ son muestras iid. de la distribución $p(x)$.\n",
    "\n",
    "Hay garantías teóricas que soportan este resultado (ver el siguiente [enlace](https://hal.archives-ouvertes.fr/hal-01216652/document) Proposition 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Porqué es importante estimar valores esperados?\n",
    "\n",
    "1. Recordamos la [siguiente clase](../../modulo1/tema4/2-enfoque-bayesiano.ipynb). En el contexto de problemas supervisados, tenemos que la predicción es:\n",
    "\n",
    "    $$\n",
    "    p(y_p | X_p, X_{tr}, y_{tr}) = \\int p(y_p | \\theta, X_p) p(\\theta | X_{tr}, y_{tr}) \\mathrm{d}\\theta,\n",
    "    $$\n",
    "\n",
    "   lo cual se puede interpretar como un valor esperado respecto a la distribución posterior $p(\\theta | X_{tr}, y_{tr}) = \\frac{p(y_{tr} | X_{tr}, \\theta) p(\\theta)}{Z}$, de la siguiente manera:\n",
    "\n",
    "    $$\n",
    "    p(y_p | X_p, X_{tr}, y_{tr}) = \\int p(y_p | \\theta, X_p) p(\\theta | X_{tr}, y_{tr}) \\mathrm{d}\\theta = \\mathbb{E}_{p(\\theta | X_{tr}, y_{tr})} [p(y_p | \\theta, X_p)].\n",
    "    $$\n",
    "    \n",
    "   **Conocemos la distribución respecto a la cual tomar el valor esperado, salvo por la constante de normalización**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De manera que, una tarea fundamental, será aquella de aprender a muestrear de diferentes distribuciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Muestreo de distribuciones univariadas\n",
    "\n",
    "### 3.1. Distribuciones discretas\n",
    "\n",
    "Sea $X$ una variable aleatoria discreta con valores $\\mathrm{Val}(X)=\\{x^1, x^2, \\dots, x^k\\}$, y $p(x^i) = \\theta^i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Cómo podemos muestrear de esta distribución?**\n",
    "\n",
    "Comúnmente tenemos a la mano un generador de números pseudoaleatorios de la distribución uniforme $\\mathcal{U}[0, 1]$ (igual probabilidad de obtener un número entre 0 y 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces, podemos hacer lo siguiente:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/discrete_sampling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta es la idea detrás de:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el ejemplo del dado de 6 caras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Distribuciones continuas\n",
    "\n",
    "Cuando hablamos de distribuciones continuas, la más común es **la distribución normal**. El muestreo de la distribución normal está bastante bien estudiado. Un método general para obtener muestras normales es el método de [Box-Muller](https://blog.cupcakephysics.com/computational%20physics/2015/05/10/the-box-muller-algorithm.html).\n",
    "\n",
    "Supongamos que $u_1$ y $u_2$ son variables aleatorias independientes que están uniformemente distribuidas en el intervalo $\\left[0,1\\right]$. Sean entonces:\n",
    "\n",
    "$$x=r\\cos(\\theta)=\\sqrt{-2\\log(u_1)}\\cos(2\\pi u_2),$$\n",
    "\n",
    "y\n",
    "\n",
    "$$y=r\\sin(\\theta)=\\sqrt{-2\\log(u_1)}\\sin(2\\pi u_2).$$\n",
    "\n",
    "Entonces, $x$ y $y$ son variables aleatorias independientes con una distribución normal estándar ($\\mathcal{N}(0,1)$).\n",
    "\n",
    "Nosotros podemos encontrar muestras de la distirbución normal usando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ¿Qué pasa si la distribución que queremos muestrear no es normal?\n",
    "\n",
    "Supongamos que queremos muestrear la siguiente distribución $p(x)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rare_pdf(x):\n",
    "    return 0.6 * norm.pdf(x, loc=-2, scale=1.5) + 0.4 * norm.pdf(x, loc=2, scale=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos basamos en que ya sabemos muestrear una normal. ¿Cómo?\n",
    "\n",
    "1. Lo primero que debemos hacer es acotar la distribución $p(x)$ con un múltiplo de una distribución normal:\n",
    "\n",
    "$$\n",
    "p(x) \\leq 2 \\mathcal{N}(x | 0, 3^2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Una vez que tenemos la distribución acotada, muestreamos de la normal:\n",
    "\n",
    "$$\n",
    "\\tilde{x} \\sim \\mathcal{N}(x | 0, 3^2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que esta muestra va a caer muy probablemente cerca a cero (la moda de la normal), sin embargo, justo en cero nuestra distribución $p(x)$ parece tener justo un mínimo local.\n",
    "\n",
    "De manera que tenemos que corregir este comportamiento, si es que queremos que nuestras muestras pertenezcan a la distribución $p(x)$. Esto lo hacemos probabilísticamente:\n",
    "\n",
    "- Aceptamos la muestra con probabilidad $\\frac{p(\\tilde{x})}{2 \\mathcal{N}(\\tilde{x} | 0, 3^2)}$. Es decir, muestreamos un número $y  \\sim \\mathcal{U}(0, 2 \\mathcal{N}(\\tilde{x} | 0, 3^2))$, y si este número es menor que $p(\\tilde{x})$ lo aceptamos, y de lo contrario, lo rechazamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tilde = norm.rvs(loc=0, scale=3)\n",
    "x_tilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, rare_pdf(x), label=\"$p(x)$\")\n",
    "plt.plot(x, bound * norm.pdf(x, loc=0, scale=3), label=\"$2 N(x|0, 3^2)$\")\n",
    "plt.fill_between([x_tilde - 1e-1, x_tilde + 1e-1], rare_pdf([x_tilde - 1e-1, x_tilde + 1e-1]), color=\"g\")\n",
    "plt.fill_between([x_tilde - 1e-1, x_tilde + 1e-1], bound * norm.pdf([x_tilde - 1e-1, x_tilde + 1e-1], loc=0, scale=3), rare_pdf([x_tilde - 1e-1, x_tilde + 1e-1]), color=\"r\")\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$p(x)$\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algoritmo completo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número de muestras\n",
    "\n",
    "# Generamos x_tilde de la normal\n",
    "\n",
    "# Muestreamos y\n",
    "\n",
    "# Corregimos muestras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¿Cuántas muestras quedan?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¡Cuidado!** Es importante elegir cotas lo más justas posibles, dado que si elegimos una cota muy por arriba de la distribución que queremos muestrear, estaremos rechazando muchas de las muestras que elijamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Número de muestras\n",
    "\n",
    "# Generamos x_tilde de la normal\n",
    "\n",
    "# Muestreamos y\n",
    "\n",
    "# Corregimos muestras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¿Cuántas muestras quedan?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos: Muy pocas muestras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siguiendo este enfoque, y suponiendo que acotamos la distribucion $p(x)$ de la siguiente manera:\n",
    "\n",
    "$$\n",
    "p(x) \\leq C q(x),\n",
    "$$\n",
    "\n",
    "estaremos aceptando alrededor de $\\frac{1}{C} M$ puntos, de los $M$ generados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Qué pasa si solo conocemos la distribución no normalizada?**\n",
    "\n",
    "El enfoque sigue siendo aplicable. Tendríamos en este caso\n",
    "\n",
    "$$\n",
    "p(x)=\\frac{\\tilde{p}(x)}{Z} \\leq C q(x) \\Rightarrow \\tilde{p}(x) \\leq \\underbrace{ZC}_{\\tilde{C}} q(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es decir, acotaríamos la distribución no normalizada y todo seguría funcionando igual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entonces...\n",
    "\n",
    "1. Tenemos un esquema general para muestrear distribuciones unidimensionales.\n",
    "\n",
    "2. Este último enfoque se puede extender a multidimensión.\n",
    "   - Sin embargo, en muchas dimensiones (más de 3) se rechazarán muchas muestras.\n",
    "   \n",
    "3. Hay que ser muy cuidadosos en el acotado de la distribución. Si elegimos una cota muy grande, estaremos desperdiciando poder de cómputo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script>\n",
    "  $(document).ready(function(){\n",
    "    $('div.prompt').hide();\n",
    "    $('div.back-to-top').hide();\n",
    "    $('nav#menubar').hide();\n",
    "    $('.breadcrumb').hide();\n",
    "    $('.hidden-print').hide();\n",
    "  });\n",
    "</script>\n",
    "\n",
    "<footer id=\"attribution\" style=\"float:right; color:#808080; background:#fff;\">\n",
    "Created with Jupyter by Esteban Jiménez Rodríguez.\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mebo2024_v4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
